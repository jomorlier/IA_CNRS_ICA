{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycFusWaBfCjy"
   },
   "source": [
    "**IA@ICA by Professor J. Morlier**\n",
    "\n",
    "From the famous example of \n",
    "Ebden, M. (2008). Gaussian processes for regression: A quick introduction. The website of robotics research group in department on engineering science, University of Oxford, 91, 424-436."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbC63MYAZwPz"
   },
   "source": [
    "**EXERCICE 1:** Given some noisy observations of a dependent variable at certain values of the independent variable , what is our best estimate of the dependent variable at a new value x* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "oqXUG15dX_3V",
    "outputId": "b382ebcf-9f60-4191-997a-4b3b45c0abb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inputs\n",
    "X = np.array([-2.0, -1.5, -1, -0.5, 0, 0.5]).reshape(-1, 1)\n",
    "n = X.shape[0]\n",
    "\n",
    "# Outputs\n",
    "y = np.array([-1.8, -1.6, -1.1, -0.5, 0.2, 0.7]).reshape(-1, 1)\n",
    "\n",
    "# Plotting data\n",
    "plt.figure()\n",
    "plt.plot(X, y, 'k.')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD22Mom2cBnZ"
   },
   "source": [
    "What if we want to estimate the model at a new point  x* defined as 18 points in [-1.5,0]. Please check also x*= 0.2 (extrapolation) !!\n",
    "The regression problem is defined as follows:\n",
    "Let $\\mathbf{x}_i \\in {R}^{6}$ be an input vector and $\\mathbf{y}_i \\in {R}^{6}$ be its corresponding target. The set of $M$ inputs are arranged into a matrix $\\mathbf{X} = [\\mathbf{x}_1, \\dots, \\mathbf{x}_M]^\\top$ and their corresponding targets are stored in a matrix $\\mathbf{Y} = [\\mathbf{y}_1 - \\mathbf{\\bar{y}}, \\dots, \\mathbf{y}_M-\\mathbf{\\bar{y}}]^\\top$, with $\\mathbf{\\bar{y}}$  being the mean target value in $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QF5c08czYaA0"
   },
   "outputs": [],
   "source": [
    "# Interpolation and extrapolation points\n",
    "# xstar > 0.5 # Extrapolation\n",
    "xstar = np.arange(-2.0, 1.01, 0.1).reshape(-1, 1)  # Adjust the range and step for better visualization\n",
    "N = xstar.shape[0]\n",
    "\n",
    "# Computing covariance natrix block structure (before Kernel) \n",
    "covXXInd1, covXXInd2 = np.meshgrid(X, X)\n",
    "covXXsInd1, covXXsInd2 = np.meshgrid(X, xstar)\n",
    "covXsXsInd1, covXsXsInd2 = np.meshgrid(xstar, xstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ6cX0F-cvIn"
   },
   "source": [
    "hey hey add something missing, we may need parameters to fit \"a model\" of our covariance matrix. Let's try with a Standard Exponential (SE) Kernel $k(x,x') =\\sigma_f^2\\exp\\left(-\\frac{(x-x')^2}{l^ 2}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8E7D-PEtcufO"
   },
   "outputs": [],
   "source": [
    "# Judicious Hyperparameters for the Kernel Function\n",
    "l = 1 # Lengthscale\n",
    "sig_f = np.sqrt(3) # Signal variance\n",
    "\n",
    "# Computing covariance matrices through SE Kernel\n",
    "#TO DO: Compute the covariance matrices\n",
    "covXX = ?\n",
    "covXsXs = ?\n",
    "covXXs = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wish to train a GPR model $\\mathcal{G} = \\lbrace \\mathbf{X}, \\mathbf{Y}, \\theta \\rbrace$ using the squared exponential function ( $\\theta$ must be chosen):\n",
    "$k(\\mathbf{x}_i, \\mathbf{x}_j) = \\sigma_f^2 \\text{exp} \\left( - \\frac{1}{l^2}(\\mathbf{x}_i - \\mathbf{x}_j)^2\\right) + \\sigma_n^2\\delta_{ij}$\n",
    ", where $\\delta_{ij}$ equals 1 if $i = j$ and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcje9S1OeB36"
   },
   "outputs": [],
   "source": [
    "# Adding noise to covariance matrix, let's try less?\n",
    "sig_n = 0.8\n",
    "covXX_noisy = covXX + sig_n ** 2 * np.eye(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "44RynqymeGY9",
    "outputId": "85089f8e-aebd-49e5-e15e-da2bb0b5489b"
   },
   "outputs": [],
   "source": [
    "# Plotting covariance matrix with and without noise\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first image on the left subplot\n",
    "im0 = axes[0].imshow(covXX_noisy, cmap='viridis')  # You can choose a colormap that suits your data\n",
    "axes[0].set_title('Noisy Covariance Matrix')\n",
    "axes[0].set_axis_off()  # Optional: Turn off axis labels and ticks\n",
    "\n",
    "# Plot the second image on the right subplot\n",
    "im1 = axes[1].imshow(covXX, cmap='viridis')  # You can choose a colormap that suits your data\n",
    "axes[1].set_title('Non Noisy Covariance Matrix')\n",
    "axes[1].set_axis_off()  # Optional: Turn off axis labels and ticks\n",
    "\n",
    "# Display the colorbars if needed\n",
    "cbar0 = fig.colorbar(im0, ax=axes[0])\n",
    "cbar1 = fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# Adjust layout to prevent clipping of titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWXxarVueG0Y"
   },
   "outputs": [],
   "source": [
    "# Computing posterior mean and covariance\n",
    "\n",
    "def posterior(covXXs,covXX,covXsXs):\n",
    "    posterior_mean = ?\n",
    "    posterior_cov  = ?\n",
    "    return posterior_mean,posterior_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "xoDmsXIcaD12",
    "outputId": "e8a30152-04d0-4689-f3c7-a0246fa9c7a1"
   },
   "outputs": [],
   "source": [
    "# Computing posterior mean and covariance without noise\n",
    "posterior_mean, posterior_cov  = posterior(covXXs,covXX,covXsXs)\n",
    "\n",
    "# Computing posterior mean and covariance with noise\n",
    "posterior_mean_noisy, posterior_cov_noisy  = posterior(covXXs,covXX_noisy,covXsXs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the mean and variance of the posterior without noise\n",
    "upper_bound = posterior_mean.flatten() + 2 * np.sqrt(np.diag(posterior_cov))\n",
    "lower_bound = posterior_mean.flatten() - 2 * np.sqrt(np.diag(posterior_cov))\n",
    "\n",
    "axes[0].fill_between(xstar.flatten(), upper_bound, lower_bound, color=[7/8, 7/8, 7/8])\n",
    "axes[0].plot(xstar, posterior_mean, 'b-', linewidth=2)\n",
    "axes[0].plot(X, y, 'k.')\n",
    "axes[0].set_title('Without noise')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "\n",
    "# Plot the mean and variance of the posterior with noise\n",
    "upper_bound_noisy = posterior_mean_noisy.flatten() + 2 * np.sqrt(np.diag(posterior_cov_noisy))\n",
    "lower_bound_noisy = posterior_mean_noisy.flatten() - 2 * np.sqrt(np.diag(posterior_cov_noisy))\n",
    "\n",
    "axes[1].fill_between(xstar.flatten(), upper_bound_noisy, lower_bound_noisy, color=[7/8, 7/8, 7/8])\n",
    "axes[1].plot(xstar, posterior_mean_noisy, 'b-', linewidth=2)\n",
    "axes[1].plot(X, y, 'k.')\n",
    "axes[1].set_title('With noise')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "FMSkXfO6YhaE",
    "outputId": "ade18a0c-9b2c-456d-9112-d5d76a6b67d3"
   },
   "outputs": [],
   "source": [
    "# Generating random function samples from posterior\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "jitter = 10 ** (-6)\n",
    "\n",
    "# Without noise\n",
    "L = np.linalg.cholesky(posterior_cov + jitter * np.eye(N))\n",
    "random_functions = posterior_mean + L.T @ np.random.randn(N, 5)\n",
    "axes[0].fill_between(xstar.flatten(), upper_bound, lower_bound, color=[7/8, 7/8, 7/8])\n",
    "axes[0].plot(xstar, random_functions,'.--')\n",
    "axes[0].set_title('Without noise')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "\n",
    "# Wiht noise\n",
    "L = np.linalg.cholesky(posterior_cov_noisy + jitter * np.eye(N))\n",
    "random_functions = posterior_mean_noisy + L.T @ np.random.randn(N, 5)\n",
    "\n",
    "axes[1].fill_between(xstar.flatten(), upper_bound_noisy, lower_bound_noisy, color=[7/8, 7/8, 7/8])\n",
    "axes[1].plot(xstar, random_functions,'.--')\n",
    "axes[1].set_title('With noise')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2QWxp5Sd21B"
   },
   "source": [
    "but is there a way to find the $\\theta_{optimal}$ ? oh $\\theta = [l, \\sigma_f, \\sigma_n]$ \n",
    "\n",
    "The hyperparameters are $\\theta = [l, \\sigma_f, \\sigma_n]$  with $\\sigma_n$ being the assumed noise level in the training data and $l$ is the length-scale  (of oscillations) and $\\sigma_f$ the amplitude.\n",
    "To train the model, I need to minimise the negative log marginal likelihood with respect to the hyperparameters:\n",
    "$-\\text{log}\\, p(\\mathbf{Y} \\mid \\mathbf{X}, \\theta) = \\frac{1}{2} \\text{tr}(\\mathbf{Y}^\\top\\mathbf{K}^{-1}\\mathbf{Y}) + \\frac{1}{2}\\text{log}\\mid\\mathbf{K}\\mid + \\,c,$\n",
    "where c is a constant and the matrix $\\mathbf{K}$ is a function of the hyperparameters (see equation k(xi,xj) = ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBN5qViYfBTr"
   },
   "source": [
    "For this purpose. Let's move to **EXERCICE 2** and use scikit learn and SMT"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
