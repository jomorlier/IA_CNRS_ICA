{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb_ZuW34AwdZ",
   "metadata": {
    "id": "fb_ZuW34AwdZ"
   },
   "source": [
    "**Professor J. Morlier**\n",
    "\n",
    "Example from \t\n",
    "Machine learning for (meta-)materials design Dr. Sid Kumar \n",
    "Delft University of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AkiUDn3l-IUw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkiUDn3l-IUw",
    "outputId": "274c9ec6-febf-4046-c57e-03afa56f9837"
   },
   "outputs": [],
   "source": [
    "pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca0615-5051-48f4-bf45-334aa48daa47",
   "metadata": {
    "id": "ccca0615-5051-48f4-bf45-334aa48daa47"
   },
   "outputs": [],
   "source": [
    "# imports:  \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0);\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd82c42-36ac-4d3a-91b4-451849ba501d",
   "metadata": {
    "id": "7cd82c42-36ac-4d3a-91b4-451849ba501d"
   },
   "source": [
    "![image.png](attachment:73423615-6ee4-42d8-8576-f3c94ba67f80.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95fad7-2c2b-463d-a9aa-966f9a40e6a8",
   "metadata": {
    "id": "9e95fad7-2c2b-463d-a9aa-966f9a40e6a8"
   },
   "source": [
    "### Input: \n",
    "$F\\ [\\text{N}], L\\ [\\text{m}], E\\ [\\text{GPa}], I\\ [\\text{cm}^4]$\n",
    "\n",
    "### Output: \n",
    "$\\delta\\ [\\text{mm}], \\theta\\ [^\\circ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14902274-1ee7-49a7-bd2b-0626c4ac83c5",
   "metadata": {
    "id": "14902274-1ee7-49a7-bd2b-0626c4ac83c5"
   },
   "outputs": [],
   "source": [
    "# Load data from .csv file using pandas (Review the code yourself)\n",
    "url = 'https://raw.githubusercontent.com/jomorlier/IA_CNRS_ICA/main/data-3AB-train.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train data (tip: convert df values to float: .astype(np.float32))\n",
    "\n",
    "x_train = torch.tensor(?)\n",
    "x_train = x_train[0::2]\n",
    "\n",
    "y_train = torch.tensor(?)\n",
    "y_train=y_train[0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cf1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jomorlier/IA_CNRS_ICA/main/data-3AB-test.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd99502-56b4-4803-b250-12c51dfd193d",
   "metadata": {
    "id": "edd99502-56b4-4803-b250-12c51dfd193d",
    "outputId": "e27fc88b-cb2b-484e-c828-912982691bcc"
   },
   "outputs": [],
   "source": [
    "# Define test data (tip: convert df values to float: .astype(np.float32))\n",
    "x_test = torch.tensor(?)\n",
    "y_test = torch.tensor(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1df225-5f6c-4fe5-b288-98ca1a409ec2",
   "metadata": {
    "id": "1b1df225-5f6c-4fe5-b288-98ca1a409ec2"
   },
   "source": [
    "## Visualize your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80f0b2-ebfd-4dad-ad56-77d162b7bb97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b80f0b2-ebfd-4dad-ad56-77d162b7bb97",
    "outputId": "f5856397-dd15-4f4c-a51c-666308352696"
   },
   "outputs": [],
   "source": [
    "# Check x-data shape\n",
    "# Please comment the data\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e1ce9-46fd-49e5-8b2a-10c13290de81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "600e1ce9-46fd-49e5-8b2a-10c13290de81",
    "outputId": "1a2a1c27-dc15-4293-e133-0d51d878a46c"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "axes[0].hist(x_train[:,0].detach().numpy(),density=True); axes[0].set_title('F');\n",
    "axes[1].hist(x_train[:,1].detach().numpy(),density=True); axes[1].set_title('L');\n",
    "axes[2].hist(x_train[:,2].detach().numpy(),density=True); axes[2].set_title('E');\n",
    "axes[3].hist(x_train[:,3].detach().numpy(),density=True); axes[3].set_title('I');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460dd78",
   "metadata": {},
   "source": [
    "What do you think about the values of different variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c84a60-a6c0-4566-a7ad-7f2d37f27118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8c84a60-a6c0-4566-a7ad-7f2d37f27118",
    "outputId": "13aa18da-c1cb-43f2-9f06-94251b2c4ce6"
   },
   "outputs": [],
   "source": [
    "# Check y-data shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec86109-395c-4641-bea7-ce207007edb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "8ec86109-395c-4641-bea7-ce207007edb0",
    "outputId": "9053ac37-fa39-44d2-c256-62aafe0c732b"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].hist(y_train[:,0].detach().numpy(),density=True,color='orange'); axes[0].set_title('delta');\n",
    "axes[1].hist(y_train[:,1].detach().numpy(),density=True,color='orange'); axes[1].set_title('theta');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813c101-22e6-492f-b07b-c0986f9ee8ab",
   "metadata": {
    "id": "f813c101-22e6-492f-b07b-c0986f9ee8ab"
   },
   "source": [
    "## Normalize your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28463ea",
   "metadata": {},
   "source": [
    "Normalization is an important step for scale consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16705b9f-7741-4b1c-a91f-97c4a965c0f9",
   "metadata": {
    "id": "16705b9f-7741-4b1c-a91f-97c4a965c0f9"
   },
   "outputs": [],
   "source": [
    "class Normalization:\n",
    "    def __init__(self,data):\n",
    "        \n",
    "        # empty list of min and max values of each column\n",
    "        self.min_values = []\n",
    "        self.max_values = []\n",
    "        \n",
    "        # find min and max of each column and store in lists\n",
    "        for i in range(data.shape[1]):\n",
    "            self.min_values.append(torch.min(data[:,i]))\n",
    "            self.max_values.append(torch.max(data[:,i]))\n",
    "        \n",
    "    def normalize(self, data):\n",
    "        \n",
    "        # clone the data before modification\n",
    "        normalized_data = data.clone()\n",
    "        \n",
    "        #iterate over all columns\n",
    "        for i in range(0, data.shape[1]):\n",
    "            \n",
    "            # z = a+b*z', where z' in [-1,1] is normalized data\n",
    "            # min_z = a-b, when z'=-1\n",
    "            # max_z = a+b, when z'=1\n",
    "            # normalize each column between values 0 and 1 range\n",
    "            a = (self.max_values[i]+self.min_values[i])/2.\n",
    "            b = (self.max_values[i]-self.min_values[i])/2.\n",
    "            normalized_data[:,i] = (data[:,i]-a)/b\n",
    "        \n",
    "        # return\n",
    "        return normalized_data\n",
    "    \n",
    "    def unnormalize(self, data):\n",
    "        \n",
    "        # clone the data before modification\n",
    "        unnormalized_data = data.clone()\n",
    "        \n",
    "        #iterate over all columns\n",
    "        for i in range(0, data.shape[1]):            \n",
    "            \n",
    "            # z = a+b*z', where z' in [-1,1] is normalized data\n",
    "            # min_z = a-b, when z'=-1\n",
    "            # max_z = a+b, when z'=1\n",
    "            # unnormalize each column to original range\n",
    "            a = (self.max_values[i]+self.min_values[i])/2.\n",
    "            b = (self.max_values[i]-self.min_values[i])/2.\n",
    "            unnormalized_data[:,i] = a + b*data[:,i]          \n",
    "            \n",
    "        # return\n",
    "        return unnormalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70845995-8f13-4f21-a102-d18a0448a9be",
   "metadata": {
    "id": "70845995-8f13-4f21-a102-d18a0448a9be"
   },
   "outputs": [],
   "source": [
    "# Always use TRAINING DATA ONLY for identifying min and max values for normalization\n",
    "x_normalization = Normalization(x_train)\n",
    "y_normalization = Normalization(y_train)\n",
    "\n",
    "# Normalize both training and test data\n",
    "x_train = x_normalization.normalize(x_train)\n",
    "x_test = x_normalization.normalize(x_test)\n",
    "\n",
    "y_train = y_normalization.normalize(y_train)\n",
    "y_test = y_normalization.normalize(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b797e-d11a-42ab-bdf7-fd34518dcd84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "ad3b797e-d11a-42ab-bdf7-fd34518dcd84",
    "outputId": "4be75d95-fb51-42a2-8a6f-05f73c6a0fa9"
   },
   "outputs": [],
   "source": [
    "# Visualize the normalized data\n",
    "fig, axes = plt.subplots(1, 6, figsize=(30, 5))\n",
    "axes[0].hist(x_train[:,0].detach().numpy(),density=True); axes[0].set_title('F');\n",
    "axes[1].hist(x_train[:,1].detach().numpy(),density=True); axes[1].set_title('L');\n",
    "axes[2].hist(x_train[:,2].detach().numpy(),density=True); axes[2].set_title('E');\n",
    "axes[3].hist(x_train[:,3].detach().numpy(),density=True); axes[3].set_title('I');\n",
    "axes[4].hist(y_train[:,0].detach().numpy(),density=True,color='orange'); axes[4].set_title('delta');\n",
    "axes[5].hist(y_train[:,1].detach().numpy(),density=True,color='orange'); axes[5].set_title('theta');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f59af1",
   "metadata": {},
   "source": [
    "## Traning the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2e3f0-3e9f-4e28-b57f-5abcddb1d61a",
   "metadata": {
    "id": "f9c2e3f0-3e9f-4e28-b57f-5abcddb1d61a"
   },
   "outputs": [],
   "source": [
    "# Create a NN model w\n",
    "nn = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,2)\n",
    ")\n",
    "\n",
    "# Define an empty list to store loss history\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Optimizer: use Adam\n",
    "optimizer = torch.optim.Adam(nn.parameters(), lr=lr)\n",
    "\n",
    "# Loss function: use the MEAN SQUARED ERROR (MSE) as loss\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5000\n",
    "\n",
    "# begin iterating over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Make your predictions, USE TRAINING DATA ONLY!!\n",
    "    y_train_pred = ?\n",
    "    \n",
    "    # Compute the loss. \n",
    "    train_loss = ?\n",
    "    \n",
    "    \n",
    "    # store the loss in a list\n",
    "    train_loss_history.append(train_loss.detach().item())\n",
    "    \n",
    "    \n",
    "    # Call .backward() on loss to compute gradient (d_loss/d_a)\n",
    "    train_loss.backward()\n",
    "    \n",
    "    # Update the model paramaeters\n",
    "    # No need for torch.no_grad() anymore with in-built optimizers\n",
    "    optimizer.step()\n",
    "    \n",
    "    # remove any pre-exisitng gradients stored\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # We also track the loss on test data but don't use that information for training (just for monitoring purpose)\n",
    "    with torch.no_grad(): #torch.no_grad() is back because we don't train on test data and so don't need gradients\n",
    "        \n",
    "        # Make your predictions\n",
    "        y_test_pred = ?\n",
    "        \n",
    "        # Compute the loss.\n",
    "        test_loss = ?\n",
    "        \n",
    "        \n",
    "        # store the loss in a list\n",
    "        test_loss_history.append(test_loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3e4ca-97da-40b7-b965-9aeddeaa366a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "3fb3e4ca-97da-40b7-b965-9aeddeaa366a",
    "outputId": "a6cc51cb-5067-4e41-9fad-c1f548f37f36"
   },
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "plt.plot(train_loss_history)\n",
    "plt.plot(test_loss_history)\n",
    "plt.yscale('log')\n",
    "plt.legend(['Train','Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8537529-284c-4a06-b873-54c9aa304664",
   "metadata": {
    "id": "f8537529-284c-4a06-b873-54c9aa304664"
   },
   "outputs": [],
   "source": [
    "def calculate_R2(true, pred):\n",
    "    # Note: both true and pred should be numpy 1D-array, NOT torch tensors\n",
    "    true_mean = true.mean()\n",
    "    ss_tot = ((true-true_mean)**2).sum()\n",
    "    ss_res = ((true-pred)**2).sum()\n",
    "    return 1. - (ss_res/ss_tot)\n",
    "\n",
    "\n",
    "def plotR2(true, pred):\n",
    "    dim = true.shape[1]\n",
    "    fig = plt.figure(figsize=plt.figaspect(1./dim))\n",
    "    def helper(ax,index):\n",
    "        R2 = calculate_R2(true[:,index].squeeze().numpy(), pred[:,index].squeeze().detach().numpy())\n",
    "        R2 = np.round(R2,2)\n",
    "        ax.scatter(true[:,index], pred[:,index].detach())\n",
    "        ax.plot([true[:,index].min(),true[:,index].max()],[true[:,index].min(),true[:,index].max()],c='red')\n",
    "        ax.set_title('Label dim: {},  R2={}'.format(index,R2))\n",
    "        ax.set_xlabel('True'); ax.set_ylabel('Predicted')\n",
    "    for i in range(dim):\n",
    "        ax = fig.add_subplot(1, dim, i+1); helper(ax,i)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7fb47-07a3-4614-bcff-a9847cb12589",
   "metadata": {
    "id": "4ed7fb47-07a3-4614-bcff-a9847cb12589"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Make prediction on test set\n",
    "    y_test_pred = nn(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74199ef-a986-4e5d-84a8-ea2233b0ab77",
   "metadata": {
    "id": "e74199ef-a986-4e5d-84a8-ea2233b0ab77"
   },
   "outputs": [],
   "source": [
    "# Unnormalize all y-data\n",
    "# To-do\n",
    "y_train     = ?\n",
    "y_test      = ?\n",
    "y_test_pred = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c73035-6647-4e17-8957-f2c4c574045c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "25c73035-6647-4e17-8957-f2c4c574045c",
    "outputId": "8aa5c100-ee92-4a11-d382-e584303bc275"
   },
   "outputs": [],
   "source": [
    "# Visualize accuracy\n",
    "plotR2(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QFye-BdVBVLU",
   "metadata": {
    "id": "QFye-BdVBVLU"
   },
   "source": [
    "**COMPARE WITH GPR using SMT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bAtoaVDk_7lM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bAtoaVDk_7lM",
    "outputId": "902fa2de-3bcf-430c-ae9f-ae65061d90ab"
   },
   "outputs": [],
   "source": [
    "from smt.surrogate_models import KRG\n",
    "\n",
    "# Load data from .csv file using pandas (Review the code yourself)\n",
    "url = 'https://raw.githubusercontent.com/jomorlier/IA_CNRS_ICA/main/data-3AB-train.csv'\n",
    "df = pd.read_csv(url)\n",
    "# Dataset is now stored in a Pandas Dataframe\n",
    "#df = pd.read_csv('data-3AB-train.csv')\n",
    "x_train = df[['F','L','E','I']].astype(np.float32).values\n",
    "# undersampling\n",
    "x_train=x_train[0::20]\n",
    "#print(x_train)\n",
    "y_train1 = df[['delta']].astype(np.float32).values\n",
    "y_train1=y_train1[0::20]\n",
    "y_train2 = df[['theta']].astype(np.float32).values\n",
    "y_train2=y_train2[0::20]\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/jomorlier/IA_CNRS_ICA/main/data-3AB-test.csv'\n",
    "df = pd.read_csv(url)\n",
    "#df = pd.read_csv('data-3AB-test.csv')\n",
    "x_test = df[['F','L','E','I']].astype(np.float32).values\n",
    "y_test1 = df[['delta']].astype(np.float32).values\n",
    "y_test2 = df[['theta']].astype(np.float32).values\n",
    "\n",
    "sm = KRG(theta0=[1e-2], poly='constant', corr='squar_exp')\n",
    "#sm = KRG(theta0=[1e-2], poly='constant', corr='matern52')\n",
    "sm.set_training_values(x_train, y_train1)\n",
    "sm.train()\n",
    "\n",
    "\n",
    "# Prediction of the validation points\n",
    "y1 = sm.predict_values(x_test)\n",
    "#print('LS,  err: '+str(compute_rms_error(y,x_test,y_test)))\n",
    "\n",
    "# Plot prediction/true values\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(y_test1, y_test1, '-', label='$y_{true}$')\n",
    "plt.plot(y_test1, y1, 'r.', label='$\\hat{y}$')\n",
    "       \n",
    "plt.xlabel('$y_{true}$')\n",
    "plt.ylabel('$\\hat{y}$')\n",
    "        \n",
    "plt.legend(loc='upper left')\n",
    "#plt.title('LS model: validation of the prediction model')\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %f\"\n",
    "      % mean_squared_error(y_test1, y1))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score (R2): %f' % r2_score(y_test1, y1))\n",
    "\n",
    "\n",
    "#surrogate2\n",
    "sm = KRG(theta0=[1e-2], poly='constant', corr='squar_exp')\n",
    "#sm = KRG(theta0=[1e-2], poly='constant', corr='matern52')\n",
    "sm.set_training_values(x_train, y_train2)\n",
    "sm.train()\n",
    "\n",
    "\n",
    "# Prediction of the validation points\n",
    "y2 = sm.predict_values(x_test)\n",
    "#print('LS,  err: '+str(compute_rms_error(y,x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5973f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalize all y-data\n",
    "#To-Do ( tip: to convert a numpy array into a pytorch tensor : torch.from_numpy())\n",
    "\n",
    "\n",
    "# Plot prediction/true values\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(y_test2, y_test2, '-', label='$y_{true}$')\n",
    "plt.plot(y_test2, y2, 'r.', label='$\\hat{y}$')\n",
    "       \n",
    "plt.xlabel('$y_{true}$')\n",
    "plt.ylabel('$\\hat{y}$')\n",
    "        \n",
    "plt.legend(loc='upper left')\n",
    "#plt.title('LS model: validation of the prediction model')\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %f\"\n",
    "      % mean_squared_error(y_test2, y2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score (R2): %f' % r2_score(y_test2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f52229-2471-4112-aa77-6a3a92b573df",
   "metadata": {
    "id": "e2f52229-2471-4112-aa77-6a3a92b573df",
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "# Constrained surrogate optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e0a70-50c1-42de-a611-fe6273f066a8",
   "metadata": {
    "id": "909e0a70-50c1-42de-a611-fe6273f066a8"
   },
   "source": [
    "Target: $\\delta=150 \\text{mm}$ ($=-0.0933$ after normalization)\n",
    "\n",
    "Given: $F = 2500 N$ ($=-0.6681$ after normalization)\n",
    "\n",
    "Find appropriate parameters: $L, E, I$ to achieve this value of target $\\delta$:\n",
    "\n",
    "### $$\\min_{L,E,I} (\\delta - \\delta_\\text{target})^2$$\n",
    "\n",
    "But normalized $L$ (also $E$, $I$) are constrained to certain bounds ($L\\in[-1,+1]$ after normalization).\n",
    "\n",
    "Therefore, set $L=\\text{tanh}(L_p)$, with $L_p\\in[-\\infty,+\\infty]$.\n",
    "\n",
    "Your job is to write the loss function\n",
    "\n",
    "Then, the problem becomes an unconstrianed optimization (which is easier to solve)\n",
    "\n",
    "### $$\\min_{L_p,E_p,I_p} (\\delta - \\delta_\\text{target})^2$$\n",
    "\n",
    "You can play with lr (learning rate), optimizer (why adam ?), activation function (why tanh? ReLU, Sigmoid ?)\n",
    "\n",
    "-----\n",
    "\n",
    "![image.png](attachment:f7258cd2-6625-4801-ba82-855a208252ec.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e727a31-8540-4dcc-9f38-75a48eb249e0",
   "metadata": {
    "id": "0e727a31-8540-4dcc-9f38-75a48eb249e0",
    "outputId": "8dd981dd-bf62-4809-e697-7f35881886e2"
   },
   "outputs": [],
   "source": [
    "given_F = -0.6681\n",
    "target_delta = -0.0933\n",
    "\n",
    "# Define L, E, I as scalar tensors with requires_grad=True and some initial guess\n",
    "L_p = torch.tensor([0.], requires_grad=True) \n",
    "E_p = torch.tensor([0.], requires_grad=True)\n",
    "I_p = torch.tensor([0.], requires_grad=True) \n",
    "\n",
    "# Define an empty list to store loss history\n",
    "loss_history = []\n",
    "\n",
    "# lr\n",
    "lr = 0.01\n",
    "\n",
    "# num_epochs\n",
    "num_epochs = 10000\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam([L_p,E_p,I_p], lr=lr)\n",
    "\n",
    "# define the tanh function\n",
    "tanh = torch.nn.Tanh()\n",
    "    \n",
    "# Lets write an optimization loop to improve the predictions and find the parameters\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Prepare input matrix\n",
    "    x = torch.zeros(1,4)\n",
    "    x[0,0] = given_F\n",
    "    x[0,1] = tanh(L_p)\n",
    "    x[0,2] = tanh(E_p)\n",
    "    x[0,3] = tanh(I_p)\n",
    "    \n",
    "    # predict y_pred\n",
    "    y_pred = nn(x)\n",
    "    \n",
    "    # Extract delta\n",
    "    delta = y_pred[0,0]\n",
    "    \n",
    "    # Compute loss as squared value of difference from target delta\n",
    "    loss = \n",
    "\n",
    "    # store the loss in a list\n",
    "    loss_history.append(loss.detach().item())\n",
    "\n",
    "    # compute gradient of loss relative to x using backward()\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Remove any existing gradients for next iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Plot loss history\n",
    "plt.plot(loss_history)\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "# Print L, E, I\n",
    "x = torch.zeros(1,4)\n",
    "x[0,0] = given_F\n",
    "x[0,1] = tanh(L_p)\n",
    "x[0,2] = tanh(E_p)\n",
    "x[0,3] = tanh(I_p)\n",
    "x = x_normalization.unnormalize(x)\n",
    "print('L:',x[0,1].detach().item())\n",
    "print('E:',x[0,2].detach().item())\n",
    "print('I:',x[0,3].detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641373fd-fa8f-42c5-a368-e8ab17b55db1",
   "metadata": {
    "id": "641373fd-fa8f-42c5-a368-e8ab17b55db1"
   },
   "source": [
    "# Moral of the story: \n",
    "\n",
    "- ### Use any PRE-TRAINED neural network as just another torch-based function that is differentiable. \n",
    "\n",
    "- ### The computational graph is automatically propagated through the neural entwork by PyTorch. \n",
    "\n",
    "- ### The differentiability can be used for any kind of optimization or even, training another neural network (as you will see in the project). \n",
    "\n",
    "- ### Using GP with lots of close related data create problems to inverse the matrix of covariance. Undersampling is a simple solution in this case.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
